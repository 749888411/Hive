
Hive性能调优指导手册
目录
1. Hive调优前相关规划设计 ............................................................................................... 4
1.1. Hive表文件使用高效的文件格式 ........................................................................ 4
1.2. Hive表文件及中间文件使用合适的文件压缩格式 ............................................ 4
1.3. 根据业务特征创建分区表 .................................................................................... 5
1.4. 根据业务特征创建分桶表 .................................................................................... 5
2. Hive调优的目标、原则及手段 ....................................................................................... 6
2.1. 调优目标 ................................................................................................................ 6
2.2. 调优原则 ................................................................................................................ 6
2.3. 调优手段 ................................................................................................................ 7
3. 关键参数配置指导 ......................................................................................................... 13
3.1. Container内存相关 ............................................................................................. 13
3.1.1. map的内存大小 ...................................................................................... 13
3.1.2. reduce的内存大小 .................................................................................. 13
3.1.3. Container的内存大小 .............................................................................. 13
3.2. Container CPU相关.............................................................................................. 14
3.2.1. map的虚拟核数 ...................................................................................... 14
3.2.2. reduce的虚拟核数 .................................................................................. 14
3.2.3. Container的虚拟核数 .............................................................................. 15
3.3. Map数量相关 ..................................................................................................... 15
3.3.1. Map处理的最大数据量 .......................................................................... 15
3.3.2. 每个节点Map分片最小值 ..................................................................... 16
3.3.3. 每个机架Map分片最小值 ..................................................................... 16
3.4. Reduce数量相关 ................................................................................................. 16
3.4.1. 每个reduce任务处理的数据量 ............................................................. 16
3.4.2. 最大reduce数 ......................................................................................... 17
3.4.3. reduce任务数量 ...................................................................................... 17
3.5. Hive优化器相关 .................................................................................................. 17
3.5.1. 相关性优化开关 ....................................................................................... 17
3.5.2. 向量化优化开关 ....................................................................................... 18
3.5.3. 基于代价的优化开关 ............................................................................... 18
4. 关键参数及HQL案例 ..................................................................................................... 19
4.1. 当输入数据量较大时减小Map处理的最大数据量 ......................................... 19
4.2. 当大量重复数据做去重时减少Reduce数量 .................................................... 19
4.3. 当大量匹配记录做关联时增加Reduce数量 .................................................... 20
4.4. 当出现Join倾斜时打开Join倾斜优化开关...................................................... 20
4.5. 当Join和Group By的字段一致时打开相关性优化开关 ................................. 21
4.6. 当Join字段和参与Join的子查询的Group By的字段一致时打开相关性优化开关 23
4.7. 对Count(Distinct)优化 ......................................................................................... 24
4.8. 使用Mutiple Insert优化 ..................................................................................... 25
4.9. 当大量表参与Join时改用MR ........................................................................... 26
5. 定位调优指导 ................................................................................................................. 28
5.1. 日志搜集 .............................................................................................................. 28
5.1.1. HiveServer日志获取 ................................................................................ 28
5.1.2. MapReduce日志查看 .............................................................................. 28
5.2. 定位调优思路 ...................................................................................................... 29
5.3. 常见问题处理 ...................................................................................................... 29
5.3.1. OOM（内存溢出） .................................................................................. 29
1. Hive调优前相关规划设计
Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供Hive SQL（简称HQL）查询功能，可以将HQL语句转换为MapReduce、Tez、Spark任务运行。本文仅讨论Hive on MapReduce的性能调优场景。
在进行Hive参数调优和SQL调优之前，要进行相应的规划设计，包括：Hive表使用高效的文件格式，Hive表文件及中间文件使用合适的文件压缩格式，根据业务特征创建分区表以及创建分桶表。
1.1. Hive表文件使用高效的文件格式
（1）建议使用ORC
ORC文件格式可以提供一种高效的方法来存储Hive数据，运用ORC可以提高Hive的读、写以及处理数据的性能。
以下两种场景需要应用方权衡是否使用ORC：
（a）文本文件加载到ORC格式的Hive表的场景：由于文本格式到ORC，需要耗费较高的CPU计算资源，相比于直接落地成文本格式Hive表而言加载性能会低很多；
（b）Hive表作为计算结果数据，导出给Hadoop之外的外部系统读取使用的场景：ORC格式的结果数据，相比于文本格式的结果数据而言其易读性低很多。
除以上两种场景外，其他场景均建议使用ORC作为Hive表的存储格式。
（2）考虑使用Parquet
Parquet的核心思想是使用“record shredding and assembly algorithm”来表示复杂的嵌套数据类型，同时辅以按列的高效压缩和编码技术，实现降低存储空间，提高IO效率，降低上层应用延迟。
Parquet是语言无关的，而且不与任何一种数据处理框架绑定在一起，适配多种语言和组件，能够与Parquet配合的组件有：
查询引擎：Hive、Impala、Pig；
计算框架：MapReduce、Spark、Cascading；
数据模型：Avro、Thrift、Protocol Buffers、POJOs。
对于Impala和Hive共享数据和元数据的场景，建议Hive表存储格式为Parquet。
1.2. Hive表文件及中间文件使用合适的文件压缩格式
GZip和Snappy，这两种压缩算法在大数据应用中最常见，适用范围最广，压缩率和速度都较好，读取数据也不需要专门的解压操作，对编码来说透明。
压缩率跟数据有关，通常从2到5不等；两种算法中，GZip的压缩率更高，但是消耗CPU更高，Snappy的压缩率和CPU消耗更均衡。
对于存储资源受限或客户要求文件必须压缩的场景，可考虑使用以上两种压缩算法对表文件及中间文件进行压缩。
1.3. 根据业务特征创建分区表
使用分区表能有效地分隔数据，分区条件作为查询条件时，减少扫描的数据量，加快查询的效率。
如果业务数据有明显的时间、区域等维度的区分，同时有较多的对应维度的查询条件时，建议按照相应维度进行一级或多级分区。
1.4. 根据业务特征创建分桶表
分桶的目的是便于高效采样和为Bucket MapJoin及SMB Join做数据准备。
对于Hive表有按照某一列进行采样稽核的场景，建议以该列进行分桶。数据会以指定列的值为key哈希到指定数目的桶中，从而支持高效采样。
对于对两个或多个数据量较大的Hive表按照同一列进行Join的场景，建议以该列进行分桶。当Join时，仅加载部分桶的数据到内存，避免OOM。
2. Hive调优的目标、原则及手段
2.1. 调优目标
Hive调优的目标是在不影响其他业务正常运行的前提下，最大限度利用集群的物理资源，如CPU、内存、磁盘IO，使其某一项达到瓶颈。如下CPU接近瓶颈：
2.2. 调优原则
（1）保证map扫描的数据量尽量少
减少map端扫描数量，需要控制待处理的表文件或中间文件的数据量尽量少。
优化的方式如：Hive表文件使用高效的文件格式、Hive表文件使用合适的文件压缩格式、中间文件使用合适的文件压缩格式、利用列裁剪、利用分区裁剪、使用分桶。
（2）保证map传送给reduce的数据量尽量小
控制map传送给reduce的数据量，是指JOIN避免笛卡尔积、启动谓词下推、开启map端聚合功能。
（3）保证map和reduce处理的数据量尽量均衡
保证map处理的数据量尽量均衡，是指使用Hive合并输入格式、必要时对小文件进行合并。
保证reduce处理的数据量尽量均衡，是指解决数据倾斜问题。包括解决group by造成的数据倾斜、解决join造成的数据倾斜。
（4）合理调整map和reduce占用的计算资源
合理调整map和reduce占用的计算资源，是指通过参数设置合理调整map和reduce的内存及虚拟核数。
根据集群总体资源情况，以及分配给当前租户的资源情况，在不影响其他业务正常运行的条件下，最大限度地利用可使用的计算资源。
（5）合理调整map和reduce的数量
合理调整map数，是指通过设置每个map处理数据量的最大和最小值来合理控制map的数量。
合理调整reduce数，是指通过直接设置reduce数量或通过设置每个reduce的处理数据量来合理控制reduce的数量。
（6）重用计算结果
重用计算结果，是指将重复的子查询结果保存到中间表，供其他查询使用，减少重复计算，节省计算资源。
（7）使用稳定成熟的Hive优化特性
使用稳定成熟的Hive优化特性，包括：相关性优化器（Correlation Optimizer），基于代价的优化（Cost-based optimization），向量化查询引擎（Vectorized Query Execution），Join相关优化（Map Join、SMB Join），Multiple Insert特性，TABLESAMPLE抽样查询、Limit优化、局部排序（SORT BY、 DISTRIBUTE BY）。
（8）使用高效HQL或改用MR
使用高效HQL，包括慎用低性能的UDF和SerDe、优化count(distinct)。
对于使用HQL比较冗余同时性能低下的场景，在充分理解业务需求后，改用MR效率更高。
2.3. 调优手段
（1）利用列裁剪
当待查询的表字段较多时，选取需要使用的字段进行查询，避免直接select *出大表的所有字段，以免当使用Beeline查询时控制台输出缓冲区被大数据量撑爆。
（2）JOIN避免笛卡尔积
JOIN场景应严格避免出现笛卡尔积的情况。参与笛卡尔积JOIN的两个表，交叉关联后的数据条数是两个原表记录数之积，对于JOIN后还有聚合的场景而言，会导致reduce端处理的数据量暴增，极大地影响运行效率。
以下左图为笛卡尔积，右图为正常Join。
（3）启动谓词下推
谓词下推（Predicate Pushdown）是一个逻辑优化：尽早的对底层数据进行过滤以减少后续需要处理的数据量。通过以下参数启动谓词下推。
（4）开启Map端聚合功能
在map中会做部分聚集操作，能够使map传送给reduce的数据量大大减少，从而在一定程度上减轻group by带来的数据倾斜。通过以下参数开启map端聚合功能。
（5）使用Hive合并输入格式
设置Hive合并输入格式，使Hive在执行map前进行文件合并，使得本轮map处理数据量均衡。通过以下参数设置Hive合并输入格式。
（6）合并小文件
启动较多的map或reduce能够提高并发度，加快任务运行速度；但同时在HDFS上生成的文件数目也会越来越多，给HDFS的NameNode造成内存上压力，进而影响HDFS读写效率。
对于集群的小文件（主要由Hive启动的MR生成）过多已造成NameNode压力时，建议在Hive启动的MR中启动小文件合并。
小文件合并能够使本轮map输出及整个任务输出的文件完成合并，保证下轮MapReduce任务map处理数据量均衡。
（7）解决group by造成的数据倾斜
通过开启group by倾斜优化开关，解决group by数据倾斜问题。
开启优化开关后group by会启动两个MR。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。
（8）解决Join造成的数据倾斜
两个表关联键的数据分布倾斜，会形成Skew Join。
解决方案是将这类倾斜的特殊值（记录数超过hive.skewjoin.key参数值）不落入reduce计算，而是先写入HDFS，然后再启动一轮MapJoin专门做这类特殊值的计算，期望能提高计算这部分值的处理速度。设置以下参数。
（9）合理调整map和reduce的内存及虚拟核数
map和reduce的内存及虚拟核数设置，决定了集群资源所能同时启动的container个数，影响集群并行计算的能力。
对于当前任务是CPU密集型任务（如复杂数学计算）的场景：在map和reduce的虚拟核数默认值基础上，逐渐增大虚拟核数进行调试（mapreduce.map.cpu.vcores和mapreduce.reduce.cpu.vcores参数控制），但不要超过可分配给container的虚拟核数（yarn.nodemanager.resource.cpu-vcores参数控制）。
对于当前任务是内存密集型任务（如ORC文件读取/写入、全局排序）的场景：在map和reduce的内存默认值基础上，逐渐增大内存值进行调试（mapreduce.map.memory.mb和mapreduce.reduce.memory.mb参数控制），但不要超过当前NodeManager上可运行的所有容器的物理内存总大小（yarn.nodemanager.resource.memory-mb参数控制）。
（10）合理控制map的数量
map的数量会影响MapReduce扫描、过滤数据的效率。
对于扫描、过滤数据的逻辑比较复杂、输入数据量较大条数较多的场景：根据集群总体资源情况，以及分配给当前租户的资源情况，在不影响其他业务正常运行的条件下，map数量需要适当增大，增加并行处理的力度。
（11）合理控制reduce的数量
reduce数量会影响MapReduce过滤、聚合、对数据排序的效率。
对于关联、聚合、排序时reduce端待处理数据量较大的场景：首先根据每个reduce处理的合适数据量控制reduce的个数，如果每个reduce处理数据仍然很慢，再考虑设置参数增大reduce个数。另一方面，控制能启动的reduce最大个数为分配给当前租户的资源上限，以免影响其他业务的正常运行。
（12）将重复的子查询结果保存到中间表
对于指标计算类型的业务场景，多个指标的HQL语句中可能存在相同的子查询，为避免重复计算浪费计算资源，考虑将重复的子查询的计算结果保存到中间表，实现计算一次、结果共享的优化目标。
（13）启用相关性优化器
相关性优化，旨在利用下面两种查询的相关性：
（a）输入相关性：在原始operator树中，同一个输入表被多个MapReduce任务同时使用的场景；
（b）作业流程的相关性：两个有依赖关系的MapReduce的任务的shuffle方式相同。
通过以下参数启用相关性优化：
相关参考：
https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer
（14）启用基于代价的优化
基于代价的优化器，可以基于代价（包括FS读写、CPU、IO等）对查询计划进行进一步的优化选择，提升Hive查询的响应速度。
通过以下参数启用基于代价的优化：
相关参考：
https://cwiki.apache.org/confluence/display/Hive/Cost-based+optimization+in+Hive
（15）启用向量化查询引擎
传统方式中，对数据的处理是以行为单位，依次处理的。Hive也采用了这种方案。这种方案带来的问题是，针对每一行数据，都要进行数据解析，条件判断，方法调用等操作，从而导致了低效的CPU利用。
向量化特性，通过每次处理1024行数据，列方式处理，从而减少了方法调用，降低了CPU消耗，提高了CPU利用率。结合JDK1.8对SIMD的支持，获得了极高的性能提升。
通过以下参数启用向量化查询引擎：
相关参考：
https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution
（16）启用Join相关优化
（a）使用MapJoin。MapJoin是针对以下场景进行的优化：两个待连接表中，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样小表复制多份，在每个map task内存中存在一份（比如存放到hash table中），然后只扫描大表。
对于大表中的每一条记录key/value，在hash table中查找是否有相同的key的记录，如果有，则连接后输出即可。
（b）使用SMB Join。
相关参考：
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization
（17）使用Multiple Insert特性
以下左图为普通insert，右图为Multiple Insert，减少了MR个数，提升了效率。
（18）使用TABLESAMPLE取样查询
在Hive中提供了数据取样（SAMPLING）的功能，用来从Hive表中根据一定的规则进行数据取样，Hive中的数据取样支持数据块取样和分桶表取样。
以下左图为数据块取样，右图为分桶表取样：
（19）启用Limit优化
启用limit优化后，使用limit不再是全表查出，而是抽样查询。涉及参数如下：
（20）利用局部排序
Hive中使用order by完成全局排序，正常情况下，order by所启动的MR仅有一个reducer，这使得大数据量的表在全局排序时非常低效和耗时。
当全局排序为非必须的场景时，可以使用sort by在每个reducer范围进行内部排序。同时可以使用distribute by控制每行记录分配到哪个reducer。
（21）慎用低性能的UDF和SerDe
慎用低性能的UDF和SerDe，主要指谨慎使用正则表达式类型的UDF和SerDe。如：regexp、regexp_extract、regexp_replace、rlike、RegexSerDe。
当待处理表的条数很多时，如上亿条，采用诸如([^ ]*)([^ ]*)([^ ]*)(.?)(\".*?\")(-|[0-9]*)(-|[0-9]*)(\".*?\")(\".*?\")这种复杂类型的正则表达式组成过滤条件去匹配记录，会严重地影响map阶段的过滤速度。
建议在充分理解业务需求后，自行编写更高效准确的UDF实现相应的功能。
（22）优化count(distinct)
优化方式如下，左图为原始HQL，右图为优化后HQL。
（23）改用MR实现
在某些场景下，直接编写MR比使用HQL更加高效。
3. 关键参数配置指导
3.1. Container内存相关
3.1.1. map的内存大小
【参数值】
mapreduce.map.memory.mb
【参数解析】
map任务的内存限制。
【如何调优】
默认：4096MB
mapreduce.map.memory.mb控制每个map任务的内存大小，默认值为4G，对于当前任务是内存密集型任务（如ORC文件读取/写入、全局排序）的场景，建议增大内存值，但不能超过yarn.scheduler.maximum-allocation-mb的值。
3.1.2. reduce的内存大小
【参数值】
mapreduce.reduce.memory.mb
【参数解析】
reduce任务的内存限制。
【如何调优】
默认：4096MB
mapreduce.reduce.memory.mb控制每个reduce任务的内存大小，默认值为4G，对于当前任务是内存密集型任务（如ORC文件读取/写入、全局排序）的场景，建议增大内存值，但不能超过yarn.scheduler.maximum-allocation-mb的值。
3.1.3. Container的内存大小
【参数值】
yarn.scheduler.maximum-allocation-mb
【参数解析】
ResourceManager中每个container请求分配的最大内存。
【如何调优】
默认：6144MB
yarn.scheduler.maximum-allocation-mb表示ResourceManager给每个container请求分配的最大内存，默认值为6G，该值属于YARN参数，需要在YARN参数配置界面进行配置。
3.2. Container CPU相关
3.2.1. map的虚拟核数
【参数值】
mapreduce.map.cpu.vcores
【参数解析】
每个map任务需要的CPU虚拟核数。
【如何调优】
默认：1
mapreduce.map.cpu.vcores控制每个map任务需要的CPU核数，默认值为1，对于当前任务是CPU密集型任务（如复杂数学计算）的场景，建议增大该值，但不能超过yarn.scheduler.maximum-allocation-vcores的值。
3.2.2. reduce的虚拟核数
【参数值】
mapreduce.reduce.cpu.vcores
【参数解析】
每个reduce任务需要的CPU虚拟核数。
【如何调优】
默认：1
mapreduce.reduce.cpu.vcores控制每个reduce任务需要的CPU核数，默认值为1，对于当前任务是CPU密集型任务（如复杂数学计算）的场景，建议增大该值，但不能超过yarn.scheduler.maximum-allocation-vcores的值。
3.2.3. Container的虚拟核数
【参数值】
yarn.scheduler.maximum-allocation-vcores
【参数解析】
ResourceManager为每个container请求的最大虚拟CPU核数。
【如何调优】
默认：32
yarn.scheduler.maximum-allocation-vcores表示ResourceManager中每个container请求的最大虚拟CPU核数，默认值是32，该值属于YARN参数，需要在YARN参数配置界面进行配置。
3.3. Map数量相关
3.3.1. Map处理的最大数据量
【参数值】
mapreduce.input.fileinputformat.split.maxsize
【参数解析】
map输入信息应被拆分成的数据块的最大大小。
【如何调优】
默认：256000000字节
对于扫描、过滤数据的逻辑比较复杂、输入数据量较大条数较多的场景，建议按照实际情况适当调小该参数的值，如128000000或是64000000，这样由于每个map处理的数据变少，map数增加了，提高了并行计算的力度，整体效率会提升。
3.3.2. 每个节点Map分片最小值
【参数值】
mapred.min.split.size.per.node
【参数解析】
每个节点的map分片的最小值。
【如何调优】
默认：1字节
对于输入数据文件存在大量小文件的场景，建议分别设置该参数的值为128000000，使map启动前对大量小文件进行合并。
3.3.3. 每个机架Map分片最小值
【参数值】
mapred.min.split.size.per.rack
【参数解析】
每个机架的map分片的最小值。
【如何调优】
默认：1字节
对于输入数据文件存在大量小文件的场景，建议分别设置该参数的值为128000000，使map启动前对大量小文件进行合并。
3.4. Reduce数量相关
3.4.1. 每个reduce任务处理的数据量
【参数值】
hive.exec.reducers.bytes.per.reducer
【参数解析】
每个reduce任务处理的数据量。
【如何调优】
默认：256000000字节
对于关联、聚合、排序时reduce端待处理数据量较大的场景，当发现reduce处理速度很慢时，建议按照实际情况适当调小该参数的值，这样由于每个reduce处理的数据变少，reduce数增加了，提高了并行计算的力度，整体效率会提升。
3.4.2. 最大reduce数
【参数值】
hive.exec.reducers.max
【参数解析】
每个MapReduce任务能启动的最大的reduce数。
【如何调优】
默认：999
建议根据集群总体资源情况，以及分配给当前租户的资源情况，设置该参数值为当前租户所能启动的最大的container数。
3.4.3. reduce任务数量
【参数值】
mapred.reduce.tasks
【参数解析】
MapReduce任务启动的reduce数。
【如何调优】
默认：-1
mapred.reduce.tasks表示reduce任务数量，默认值为-1，表示由Hive根据输入数据量除以hive.exec.reducers.bytes.per.reducer参数的值自动计算reduce的任务数量。
该值的优先级高于根据hive.exec.reducers.bytes.per.reducer参数的自动计算。
对于关联、聚合、排序时reduce端待处理数据量较大的场景，当发现reduce处理速度很慢时，建议按照实际情况调高该参数的值，但不能超过hive.exec.reducers.max的值。
3.5. Hive优化器相关
3.5.1. 相关性优化开关
【参数值】
hive.optimize.correlation
【参数解析】
是否开启相关性优化。
【如何调优】
默认：false
相关性优化，旨在利用下面两种查询的相关性：
（a）输入相关性：在原始operator树中，同一个输入表被多个MapReduce任务同时使用的场景；
（b）作业流程的相关性：两个有依赖关系的MapReduce的任务的shuffle方式相同。
相关性优化特性可以减少MR任务个数，从而提升整体效率，建议设置为true。
3.5.2. 向量化优化开关
【参数值】
hive.vectorized.execution.enabled
【参数解析】
是否开启向量化优化。
【如何调优】
默认：false
向量化特性可以显著地减少如下几类查询的CPU使用率，如扫描、过滤、聚合和关联，建议设置为true。
3.5.3. 基于代价的优化开关
【参数值】
hive.cbo.enable
【参数解析】
是否开启基于代价的优化。
【如何调优】
默认：true
基于代价的优化器，可以基于代价（包括FS读写、CPU、IO等）对查询计划进行进一步的优化选择，提升Hive查询的响应速度，建议设置为true。
4. 关键参数及HQL案例
4.1. 当输入数据量较大时减小Map处理的最大数据量
已知表midsrc有1.5亿条记录，如下：
分别设置map处理最大数据量为1024000000、512000000、256000000、128000000观察以下语句的执行情况。
统计信息如下：
Map处理的最大数据量
Mapper数
执行时长（秒）
1024000000
5
117.098
512000000
9
67.62
256000000
17
52.739
128000000
33
49.971
可以看到：随着map处理最大数据量变小，map数逐渐增大，整体效率提升。在最大数据量从256000000到128000000时，差别已经不大了。
4.2. 当大量重复数据做去重时减少Reduce数量
已知文本表hugest有9.5亿条记录，如下：
使用Hive默认参数执行以下语句查询不重复的记录。
共启动105个mapper，110个reducer，耗时137.394秒。
最终发现仅有三条不重复记录，由于map端已聚合，可推测reducer只要一个就够了，会大大减少reduce阶段耗时。
设置以下参数后再次执行语句。
共启动105个mapper，1个reducer，耗时98.524秒，效果明显。
4.3. 当大量匹配记录做关联时增加Reduce数量
已知表dynhuge和dynhuge1有784万条记录，使用Hive默认参数执行以下语句关联两表并将数据导入表tmp。
共启动2个mapper，1个reducer，耗时10个小时左右。
观察发现由于两表关联字段seq存在很多相同值，关联产生巨大的数据，一个reducer处理起来非常慢，按照当前集群和租户的能力，最大可启动56个container，因此考虑设置reducer个数为50。
设置以下参数后再次执行语句。
共启动2个mapper，50个reducer，耗时16分钟，效果明显。
4.4. 当出现Join倾斜时打开Join倾斜优化开关
表skewtable有392329条记录，表unskewtable有129927条记录。
使用Hive默认参数执行以下语句：
启动一个MR，耗时215.742秒，同时主要耗费在reduce阶段。
经统计，发现表skewtable有记录A的数量131650，表unskewtable有记录A的数量449，可知两表关联将会出现一定的数据倾斜。
设置join倾斜优化开关，再次执行如下：
启动了两个MR，总耗时157.494秒，单独启动一个MR处理了倾斜数据后，效率提升了。
4.5. 当Join和Group By的字段一致时打开相关性优化开关
表log有305872条记录，表logcopy有295825条记录。
使用Hive默认参数执行以下语句：
启动了两个MR，总耗时64.59秒
由于join的字段和group by字段均为key，可以利用相关性优化，减少MR个数从而提高运行速度。
设置以下参数后，再次执行：
只启动一个MR，总耗时32.678秒。
4.6. 当Join字段和参与Join的子查询的Group By的字段一致时打开相关性优化开关
表log有305872条记录，表logcopy有295825条记录。
使用Hive默认参数执行以下语句：
启动了三个MR，总耗时98.714秒。
由于子查询的group by字段和join字段一致，可以利用相关性优化，减少MR个数从而提高运行速度。
设置以下参数后，再次执行：
只启动一个MR，耗时33.855秒，效果明显。
4.7. 对Count(Distinct)优化
ORC表orctbl有78914976条记录，表大小1.6G。
使用Hive默认参数执行以下语句：
由于是全聚合，只启动一个MR，总耗时295.997秒。
优化后可以将reduce数增大（测试集群可启动的container数为51），执行如下：
启动了两个MR，总耗时198.123秒。
4.8. 使用Mutiple Insert优化
表src有500条记录。
分别执行插入操作，第一个插入操作如下：
第一个插入操作耗时30.536秒。
第二个插入操作如下：
第二个插入操作总耗时26.257秒。
两个SQL分别启动两个MR，共耗时56.793秒。
优化如下：
只启动一个MR，耗时27.187秒，效果提升明显。
4.9. 当大量表参与Join时改用MR
如下场景，需要将用户信息表USER与INDICT_1、INDICT_2、INDICT_3、INDICT_4、INDICT_5等一定数量的指标表进行关联，目标是汇总用户的所有指标到一个新的用户指标表，一方面SQL比较冗长，另一方面由于多次join性能较低。同时后续还需要加入更多同类型的指标表参与连接，届时还需要修改SQL才能完成相应功能。
了解业务需求后，考虑使用直接编写MR实现，MAP的输入为用户信息表USER及所有指标表的目录下的文件，MAP输出为用户ID、指标值，REDUCE输入为用户ID、指标值序列，REDUCE输出为用户ID和按顺序排列的指标值，落地成结果文件。MR程序能做到指标可配置（可配置文件目录名与指标名的映射），扩展性好（不断新增指标只需改配置文件，无须修改代码/SQL），效率更高（一个MR完成指标汇总的所有功能）。
5. 定位调优指导
5.1. 日志搜集
5.1.1. HiveServer日志获取
Hive调优需要看HiveServer的运行日志及GC日志。
HiveServer日志路径为：HiveServer节点的/var/log/Bigdata/hive/hiveserver/。 文件名 日志内容
hive.log
HiveServer运行日志
hive-omm-gc.log.0.current
HiveServer GC日志
通过Manager界面也可以下载，如下：
5.1.2. MapReduce日志查看
Hive提交的MapReduce的日志中详细地记录了map和reduce任务的运行情况。
通过Yarn原生界面可以查看，如下：
5.2. 定位调优思路
第一步，分析SQL待处理的表及文件，统计待处理的表的文件数、数据量、条数、文件格式、压缩格式，分析是否有更适合的文件存储格式、压缩格式，是否能够使用上分区或分桶，是否有大量小文件需要map前合并。如果有优化空间，则执行优化。
第二步，分析SQL的结构，是否有重复的子查询可以存到中间表，是否可以使用相关性优化器，是否出现笛卡尔积需要去除，是否可以使用Multiple Insert语句，是否使用了低性能的UDF或SerDe需要替换。如果有优化空间，则执行优化。
第三步，分析SQL的操作，是否可利用MapJoin，是否可使用SMB Join，是否需要设置map端聚合，是否需要优化count(distinct)，是否全局排序可以使用局部排序代替，是否可以使用向量化优化、基于代价的优化等优化器。如果有优化空间，则执行优化。
第四步，观察SQL启动的MR运行情况，如果map运行缓慢，考虑减小Map处理的最大数据量提高并发度，考虑增大map的内存和虚拟核数；如果是reduce运行缓慢，是否有group by倾斜需要解决，是否有join倾斜需要处理，当大量重复数据做去重时减少Reduce数量，当大量匹配记录做关联时增加Reduce数量。
5.3. 常见问题处理
5.3.1. OOM（内存溢出）
5.3.1.1. 问题描述
HiveServer运行日志及MR日志报以下错误：
Container [pid=48338,containerID=container_1417574466181_3484_01_000043] is running beyond physical memory limits. Current usage: 1.0 GB of 1 GB physical memory used; 2.0
GB of 2.1 GB virtual memory used. Killing container. Dump of the process-tree for container_1417574466181_3484_01_000043 : |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE |- 48345 48338 48338 48338 (java) 2432 61 2062110720 272981
5.3.1.2. 解决方案
从日志可以看出，Container物理内存不够用了，观察YARN界面该container是map还是reduce。
如果是map，则增大mapreduce.map.memory.mb的设置，如设置成6144（默认值为4096），但不能超过yarn.scheduler.maximum-allocation-mb的值。
如果是reduce，则增大mapreduce.reduce.memory.mb的设置，如设置成6144（默认值为4096），但不能超过yarn.scheduler.maximum-allocation-mb的值。
